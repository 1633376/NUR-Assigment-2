\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A plot of random number $x_{i+1}$ against random number $x_{i}$ for the first 1000 random uniforms produced by the random number generator. A good random number generator should produce a homogeneous plot without many (large) empty spots. The largest empty spot in the above plot is at $x_i$ = 0.4 and $x_{i+1}$ = 0.8. The spot is not significant large, but might point towards an impurity in the RNG. }}{3}{figure.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The first 1000 random uniform numbers produced by the random number generator (RNG) against their index. A good random number generator should not have large wide gaps (e.g when moving from index 400 to 450 it should not only produce values larger than 0.8, which would leave a wide gap). In the plot small gaps appear, see for example index $\sim 420$ at a probability of $ p = 0.6$. The number of gaps and the width of the gaps do not appear to be significant. This might therefore either be the result of being unlucky, or could point towards an impurity in the RNG. The average value produced by the RNG should furthermore be 0.5. This corresponds to rapidly moving up and down around the horizontal line corresponding with a probability of $ p = 0.5$. In the plot this should, result in a 'dense' region (less white) around the line $p = 0.5$. It can indeed be seen that the plot is denser around the line $p = 0.5$ than at $ p = 0.8$ or $p = 0.2$. }}{3}{figure.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The uniforms of the random number generator for 1 million random values. The values are binned in 20 bins. A good random number generator should fluctuate around 50000 $\pm 2\sqrt  {50000} = 50000 \pm 447 $ counts per bin (2 sigma). The maximum and minimum amount of counts corresponds to 50343 and 49557 counts. These values just lay withing the 2 sigma uncertainty. The uniformness of the random number generator therefore appears to be acceptable. }}{4}{figure.3}}
\newlabel{EQ:boxmuller}{{9}{5}{\textbf {Question 1.b)}}{equation.0.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces A histogram of the 1000 random normal distributed variables generated with the box muller method for $\mu = 3$ and $\sigma = 2.4$ (orange). The red line is the true normal distribution. The histogram appears to approximate the distribution quite well, but displays small deviations. The bin left of the peak (the highest bin) is larger than it should be. The first bins right of the peak is smaller than it should be and the second bin right of the peak is to high. The histogram does however still appear to be acceptable by eye. A statistical test is of course better to determine whether the histogram would truly be acceptable or not.}}{6}{figure.4}}
\newlabel{fig:normal}{{4}{6}{A histogram of the 1000 random normal distributed variables generated with the box muller method for $\mu = 3$ and $\sigma = 2.4$ (orange). The red line is the true normal distribution. The histogram appears to approximate the distribution quite well, but displays small deviations. The bin left of the peak (the highest bin) is larger than it should be. The first bins right of the peak is smaller than it should be and the second bin right of the peak is to high. The histogram does however still appear to be acceptable by eye. A statistical test is of course better to determine whether the histogram would truly be acceptable or not}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The P-value produced by the KS-test against the number of samples on which the KS-test is performed for the self written RNG. The red line indicates the line of $ p = 0.05$. A point \textbf  {below} the line would suggests that there is enough statistical evidence to reject the (null) hypothesis that the data is normal distributed. The plot shows that the RNG always passes KS-test up to atleast $10^5$ samples. The p-value does however appear to drop for a large number of samples and might even drop further when more samples are used. The drop suggests again that the RNG is likely not perfect.}}{9}{figure.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The P-value produced by the KS-test against the number of samples on which the KS-test is performed for the self written RNG. The red line indicates the line of $ p = 0.05$. The orange line is the self written implementation of the KS-test and the blue line is the scipy version. A point \textbf  {below} the red line would suggests that there is enough statistical evidence to reject the (null) hypothesis that the data is normal distributed. The self written KS-test is close to the scipy version, but shows (small) deviations at small sample sizes (see $N_{samples} = 10$ or $N_{samples} = 200$). The self written implementation always has the same shape as the scipy version, even at the deviations. The exact cause for the deviations are unknown, but are likely the result of an approximation that scipy makes that the self written implementation doesn't make. }}{9}{figure.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The P-value produced by the Kuiper-test against the number of samples on which the kuiper-test is performed for the self written RNG. The red line indicates the line of $ p = 0.05$. A point \textbf  {below} there line would suggests that there is enough statistical evidence to reject the (null) hypothesis that the data is normal distributed. The plot shows that the RNG always passes Kuiper test. It can however be seen that the p-value drops for larger sample size, similar as what happens by the KS-test. This might, as mentioned before, indicate that there is an flaw in the random number generator.}}{11}{figure.7}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces The P-value produced by the kuiper test against the number of samples on which the kuiper-test is performed for the self written RNG. The red line indicates the line of $ p = 0.05$. A point \textbf  {below} there line would suggests that there is enough statistical evidence to reject the (null) hypothesis that the data is normal distributed. The plot shows that the self written implementation has (small) deviations from the astropy implementation at small sample sizes. This is similar to the situation with the KS-test and might be caused by an approximation made in astropy. }}{12}{figure.8}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The P-value produced by performing the KS-test2 for a normal distribution with $\mu = 0$ and $\sigma = 1$ on the \textbf  {first} column. The red line indicates the line of $ p = 0.05$. A point \textbf  {below} there line would suggests that there is enough statistical evidence to reject the (null) hypothesis that the data is normal distributed. The plot shows that the p-value drops below 0.05 when including all samples. There is thus enough statistical evidence to reject the hypothesis that this column is normal distributed with $\mu = 0$ and $\sigma = 1$.}}{14}{figure.9}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces The P-value produced by performing the KS-test2 for a normal distribution with $\mu = 0$ and $\sigma = 1$ on the \textbf  {second} column. The red line indicates the line of $ p = 0.05$. A point \textbf  {below} there line would suggests that there is enough statistical evidence to reject the (null) hypothesis that the data is normal distributed. The plot shows that the p-value drops below 0.05 when including all samples. There is thus enough statistical evidence to \textbf  {reject} the hypothesis that this column is normal distributed with $\mu = 0$ and $\sigma = 1$.}}{14}{figure.10}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces The P-value produced by performing the KS-test2 for a normal distribution with $\mu = 0$ and $\sigma = 1$ on the \textbf  {third} column. The red line indicates the line of $ p = 0.05$. A point \textbf  {below} there line would suggests that there is enough statistical evidence to reject the (null) hypothesis that the data is normal distributed. The plot shows that the p-value drops below 0.05 when including all samples. There is thus enough statistical evidence to \textbf  {reject} the hypothesis that this column is normal distributed with $\mu = 0$ and $\sigma = 1$.}}{15}{figure.11}}
\newlabel{FIG:Good}{{}{15}{\textbf {Question 1.e)}}{figure.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces The P-value produced by performing the KS-test2 for a normal distribution with $\mu = 0$ and $\sigma = 1$ on the \textbf  {fourth} column. The red line indicates the line of $ p = 0.05$. A point \textbf  {below} there line would suggests that there is enough statistical evidence to \textbf  {reject} the (null) hypothesis that the data is normal distributed. The plot shows that the p-value drops below 0.05 only between $500-1000$ samples. In all other cases it passes the KS-test. The most important point in this plot is the p-value when all samples are included. It can be clearly seen that this point isn't below the red line. The plot therefore suggests that the given c olumn might( see footnote 2 on page 12) be a Gaussian with $\mu = 0$ and $\sigma =1$. }}{15}{figure.12}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces The P-value produced by performing the KS-test2 for a normal distribution with $\mu = 0$ and $\sigma = 1$ on the \textbf  {fifth} column. The red line indicates the line of $ p = 0.05$. A point \textbf  {below} there line would suggests that there is enough statistical evidence to reject the (null) hypothesis that the data is normal distributed. The plot shows that the p-value drops below 0.05 when including all samples. There is thus enough statistical evidence to reject the hypothesis that this column is normal distributed with $\mu = 0$ and $\sigma = 1$.}}{16}{figure.13}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces The P-value produced by performing the KS-test2 for a normal distribution with $\mu = 0$ and $\sigma = 1$ on the \textbf  {sixth} column. The red line indicates the line of $ p = 0.05$. A point \textbf  {below} there line would suggests that there is enough statistical evidence to reject the (null) hypothesis that the data is normal distributed. The plot shows that the p-value drops below 0.05 and stays there when including more than halve of the samples. The most important part of the figure is the part that includes most samples. There is thus enough statistical evidence to reject the hypothesis that this column is normal distributed with $\mu = 0$ and $\sigma = 1$.}}{16}{figure.14}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces The P-value produced by performing the KS-test2 for a normal distribution with $\mu = 0$ and $\sigma = 1$ on the \textbf  {seventh} column. The red line indicates the line of $ p = 0.05$. A point \textbf  {below} there line would suggests that there is enough statistical evidence to reject the (null) hypothesis that the data is normal distributed. The plot shows that the p-value drops below 0.05 when including all samples. There is thus enough statistical evidence to reject the hypothesis that this column is normal distributed with $\mu = 0$ and $\sigma = 1$.}}{17}{figure.15}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces The P-value produced by performing the KS-test2 for a normal distribution with $\mu = 0$ and $\sigma = 1$ on the \textbf  {eight} column. The red line indicates the line of $ p = 0.05$. A point \textbf  {below} there line would suggests that there is enough statistical evidence to reject the (null) hypothesis that the data is normal distributed. The plot shows that the p-value drops below 0.05 when including all samples. There is thus enough statistical evidence to reject the hypothesis that this column is normal distributed with $\mu = 0$ and $\sigma = 1$.}}{17}{figure.16}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces The P-value produced by performing the KS-test2 for a normal distribution with $\mu = 0$ and $\sigma = 1$ on the \textbf  {ninth} column. The red line indicates the line of $ p = 0.05$. A point \textbf  {below} there line would suggests that there is enough statistical evidence to reject the (null) hypothesis that the data is normal distributed. The plot shows that the p-value drops below 0.05 when including all samples. There is thus enough statistical evidence to reject the hypothesis that this column is normal distributed with $\mu = 0$ and $\sigma = 1$.}}{18}{figure.17}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces The P-value produced by performing the KS-test2 for a normal distribution with $\mu = 0$ and $\sigma = 1$ on the \textbf  {tenth} column. The red line indicates the line of $ p = 0.05$. A point \textbf  {below} there line would suggests that there is enough statistical evidence to reject the (null) hypothesis that the data is normal distributed. The plot shows that the p-value drops below 0.05 when including all samples. There is thus enough statistical evidence to reject the hypothesis that this column is normal distributed with $\mu = 0$ and $\sigma = 1$.}}{18}{figure.18}}
\newlabel{CODE:MAIN1}{{}{19}{\textbf {Question 1- Summary}}{section*.7}{}}
\newlabel{CODE:RNG}{{}{19}{\textbf {Question 1- Summary}}{lstnumber.-7.18}{}}
\newlabel{CODE:Statistics}{{}{22}{\textbf {Question 1- Summary}}{lstnumber.-8.207}{}}
\newlabel{CODE:Sorting}{{}{27}{\textbf {Question 1- Summary}}{lstnumber.-10.67}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces The Gaussian field for $n = -1$ and a minimal physical size of 1 Mpc. The power drops for $n = -1$ slowly, as result, mainly noise is expected. The plot seems to show that this is the case.}}{32}{figure.19}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces The Gaussian field for $n = -2$ and a minimal physical size of 1 Mpc. The plot seems to be less noisy than the previous plot, which is expected as $n$ is now smaller. }}{32}{figure.20}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces The Gaussian field for $n = -3$ and a minimal physical size of 1 Mpc. High and low density fluctuations are now clearly visible. Notice that the plot looks similar to the plot for $n = -2 $. This is a consequence of using the same random uniform variables in the Box-Muller method. }}{33}{figure.21}}
\newlabel{eq:ode}{{18}{33}{\textbf {Question 3}}{equation.0.18}{}}
\newlabel{eq:ode2}{{21}{34}{\textbf {Question 3}}{equation.0.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces The analytical (blue) and numerical (orange) solution of the ODE with initial conditions $D(1) = 3, D'(1) = -10$. The plots show that he numerical solution does not appear to have a visible deviation from the analytical solution for the given time interval.}}{38}{figure.22}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces The analytical (blue) and numerical solution (orange) of the ODE with initial conditions $D(1) = 10, D'(1) = -10$. The plots show that he numerical solution does not appear to have a visible deviation from the analytical solution for the given time interval.}}{38}{figure.23}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces The analytical (blue) and numerical (orange) solution of the ODE with initial conditions $D(1) = 5, D'(1) = 0$. The plots show that he numerical solution does not appear to have a visible deviation from the analytical solution for the given time interval.}}{39}{figure.24}}
\newlabel{eq:lg}{{27}{39}{\textbf {Question 4.a)}}{equation.0.27}{}}
\newlabel{eq:Dd}{{30}{39}{\textbf {Question 4.a)}}{equation.0.30}{}}
\newlabel{EQ:Stuffffff}{{33}{40}{\textbf {Question 4.a)}}{equation.0.33}{}}
\newlabel{EQ:weird}{{34}{41}{\textbf {Question 4.b)}}{equation.0.34}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces The final plot of the movie. It can clearly be seen that the particles (black dots) moved to the denser region. }}{47}{figure.25}}
\newlabel{FIG:stuff}{{25}{47}{The final plot of the movie. It can clearly be seen that the particles (black dots) moved to the denser region}{figure.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces The y-positions of the first 10 particles as function of the scale factor. The plot shows that the particles are slowly moving to a denser region (see figure \ref  {FIG:stuff}, where the first 10 particles are at the left top). The large jump for particle 2 (orange) at a scale factor of around $ a \approx 0.8$ is the result of the circular boundary conditions. }}{47}{figure.26}}
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces The momentum of the first 10 particles as function of the scale factor. The fact that about halve of the particles have a positive momentum and that the other halve have a negative momentum is the result of the circular boundary conditions and the created field. }}{48}{figure.27}}
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces The z-positions of the first 10 particles against the scale factor.}}{52}{figure.28}}
\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces The z-component of the momentum of the first 10 particles against the scale factor. }}{52}{figure.29}}
\newlabel{CODE:MAIN4}{{}{53}{\textbf {Question 4 - Summary}}{section*.17}{}}
\newlabel{CODE:h4}{{}{53}{\textbf {Question 4 - Summary}}{lstnumber.-22.32}{}}
\newlabel{CODE:misc}{{}{57}{\textbf {Question 4 - Summary}}{lstnumber.-24.205}{}}
\newlabel{CODE:misc}{{}{58}{\textbf {Question 4 - Summary}}{lstnumber.-25.74}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {30}{\ignorespaces The x-y slice of the created mass grid for $z = 4$. The color indicates the assigned mass in terms of particle mass. }}{60}{figure.30}}
\@writefile{lof}{\contentsline {figure}{\numberline {31}{\ignorespaces The x-y slice of the created mass grid for $z = 9$. The color indicates the assigned mass in terms of particle mass. Notice that the range of the colorbar is different from the first and last plot.}}{60}{figure.31}}
\@writefile{lof}{\contentsline {figure}{\numberline {32}{\ignorespaces The x-y slice of the created mass grid for $z = 11$. The color indicates the assigned mass in terms of particle mass. Notice that the range of the colorbar is different than the first and last plot. }}{61}{figure.32}}
\@writefile{lof}{\contentsline {figure}{\numberline {33}{\ignorespaces The x-y slice of the created mass grid for $z = 14$. The color indicates the assigned mass in terms of particle mass. }}{61}{figure.33}}
\@writefile{lof}{\contentsline {figure}{\numberline {34}{\ignorespaces The mass assigned for a particle moving from x = 0 to x = 16. The orange and red line respectively indicate the mass assigned to cells 0 and 4 as function of the position of the particle. The plot is created for a mass grid of size 16 with circular boundary conditions. }}{63}{figure.34}}
\@writefile{lof}{\contentsline {figure}{\numberline {35}{\ignorespaces The x-y slice of the created mass grid with the CIC method for $z = 4$. The color indicates the assignment mass in terms of particle mass. Notice that the range of the colorbar deviates from the other three figures below.}}{65}{figure.35}}
\@writefile{lof}{\contentsline {figure}{\numberline {36}{\ignorespaces The x-y slice of the created mass grid with the CIC method for $z = 9$. The color indicates the assignment mass in terms of particle mass. Notice that the scalre of the colorbar deviates from the other three figures.}}{65}{figure.36}}
\@writefile{lof}{\contentsline {figure}{\numberline {37}{\ignorespaces The x-y slice of the created mass grid with the CIC method for $z = 11$. The color indicates the assignment mass in terms of particle mass. Notice that the colorbar deviates from the other three figures. }}{66}{figure.37}}
\@writefile{lof}{\contentsline {figure}{\numberline {38}{\ignorespaces The x-y slice of the created mass grid with the CIC for $z = 14$. The color indicates the assignment mass in terms of particle mass.Notice that the scale of the colorbar deviates from the other three figures. }}{66}{figure.38}}
\@writefile{lof}{\contentsline {figure}{\numberline {39}{\ignorespaces The mass assigned for a particle moving from x = 0 to x = 16 for the cloud in the cell method. The orange and red line respectively indicate the mass assigned to cells 0 and 4 as function of the position of the particle. The plot is created for a 3D mass grid of size 16 with circular boundary conditions. }}{67}{figure.39}}
\@writefile{lof}{\contentsline {figure}{\numberline {40}{\ignorespaces The own implementation of the FFT (orange), the numpy implementation (blue) and the analytical fft (black). The plot shows that there is now visible deviation from the numpy version. It can also be seen that both the numpy and the self written implementation do not correctly represent the peak of the delta function. This is expected as it would require an infinite amount of samples to obtain the exact same result. }}{69}{figure.40}}
\@writefile{lof}{\contentsline {figure}{\numberline {41}{\ignorespaces The 2D fourier transformation for the function $\qopname  \relax o{cos}(x+y)$. Left, the own implementation of the FFT2 (left) and right the numpy implementation. The results show that the self written version appears to be equal to the numpy version. For the cosine two delta peaks are expected to arise which can also be seen. The peaks are not infinite sharp as that would require an infinite amount of samples. }}{71}{figure.41}}
\@writefile{lof}{\contentsline {figure}{\numberline {42}{\ignorespaces The yz slice around the center of the 3D Fourier transform of a multivariate gaussian with $\mu = 0$ and $\sigma = 0.5$. The slice should be perfectly symmetric and the result shows that this is indeed the case. }}{71}{figure.42}}
\@writefile{lof}{\contentsline {figure}{\numberline {43}{\ignorespaces The xz slice around the center of the 3D Fourier transform of a multivariate gaussian with $\mu = 0$ and $\sigma = 0.5$. The slice should be perfectly symmetric and the result shows that this is indeed the case. }}{72}{figure.43}}
\@writefile{lof}{\contentsline {figure}{\numberline {44}{\ignorespaces The xy slice around the center of the 3D Fourier transform of a multivariate gaussian with $\mu = 0$ and $\sigma = 0.5$. The slice should be perfectly symmetric and the result shows that this is indeed the case. }}{72}{figure.44}}
\newlabel{CODE:MAIN5}{{}{72}{\textbf {Question 5 - Summary}}{section*.24}{}}
\newlabel{CODE:mass}{{}{73}{\textbf {Question 5 - Summary}}{lstnumber.-31.12}{}}
\newlabel{CODE:fourier}{{}{76}{\textbf {Question 5 - Summary}}{lstnumber.-33.87}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {45}{\ignorespaces The created histogram for the prediction made by the model and the true labels. It can be seen here that the model always returns '1'. }}{80}{figure.45}}
\@writefile{lof}{\contentsline {figure}{\numberline {46}{\ignorespaces A visual representation of the constructed quadtree. The blue points indicate the positions of the bodies added to the tree. The red point indicates the position of the body with index 100.}}{86}{figure.46}}
